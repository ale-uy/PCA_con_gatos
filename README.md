![image](https://raw.githubusercontent.com/ale-uy/PCA_con_gatos/refs/heads/main/5.png)

[ENGLISH]

## Ever had PCA explained with complex formulas and still felt a bit lost? Sometimes, a picture explains it all.

This is what Principal Component Analysis (PCA) actually looks like in action.

We take an original image of a cat (top left) and break it down into its fundamental mathematical "ingredients" â€“ the principal components.

ðŸ”¹ With just 1 component: You can barely make out the cat's essence. It's a "ghostly" version that captures the most variance.
ðŸ”¹ As we add more components (5, 10, 20...): The reconstructed image starts to regain details like the eyes, muzzle, and fur textures. It gets sharper and sharper!
ðŸ”¹ With only 30 of the original 4096 components!: The reconstructed image is nearly identical to the original.

This is dimensionality reduction at its best: we've represented an image with less than 1% of its original information, with almost no loss in visual quality!
What's this useful for? Compressing data, speeding up Machine Learning algorithms, and visualizing complex information.

If you'd like to see the full, step-by-step breakdown of this process (with code and explanations), I've prepared a PDF summary you can check out. Hope you find it useful!

### [link](https://github.com/ale-uy/PCA_con_gatos/raw/main/PCA_en.pdf)

#DataScience #MachineLearning #PCA #ArtificialIntelligence #PrincipalComponentAnalysis #Python #DataVisualization #AI

---

[ESPAÃ‘OL]

## Â¿Te han explicado PCA con fÃ³rmulas complejas y no has terminado de entenderlo? A veces, una imagen lo explica todo.

AsÃ­ es como se ve realmente el AnÃ¡lisis de Componentes Principales (PCA) en acciÃ³n.

Tomamos una imagen original de un gato (arriba a la izquierda) y la descomponemos en sus "ingredientes" matemÃ¡ticos fundamentales, los componentes principales.

ðŸ”¹ Con solo 1 componente: Apenas se distingue la esencia del gato. Es una versiÃ³n "fantasma" que captura la mÃ¡xima varianza.
ðŸ”¹ A medida que agregamos mÃ¡s componentes (5, 10, 20...): La imagen reconstruida recupera detalles como los ojos, el hocico y las texturas del pelaje. Â¡Se vuelve cada vez mÃ¡s nÃ­tida!
ðŸ”¹ Â¡Con solo 30 de los 4096 componentes originales!: La imagen reconstruida es casi idÃ©ntica a la original.

Esto es la reducciÃ³n de dimensionalidad en su mÃ¡xima expresiÃ³n: hemos representado una imagen con menos del 1% de su informaciÃ³n original, Â¡sin perder casi nada de calidad visual!

Â¿Para quÃ© sirve esto? Para comprimir datos, acelerar algoritmos de Machine Learning y visualizar informaciÃ³n compleja.

Si quieres ver el desglose completo de este proceso paso a paso (con cÃ³digo y explicaciones), preparÃ© un resumen en PDF que puedes revisar. Â¡Espero que te sea Ãºtil!

### [Link](https://github.com/ale-uy/PCA_con_gatos/raw/main/PCA_es.pdf)

#DataScience #MachineLearning #PCA #InteligenciaArtificial #AnalisisDeComponentesPrincipales #Python #VisualizacionDeDatos #AI
